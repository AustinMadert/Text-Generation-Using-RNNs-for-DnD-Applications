{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textGen_Keras_Generator.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3kr5RU1p2rC",
        "colab_type": "text"
      },
      "source": [
        "# Text Generation Using RNNs for D&D Applications\n",
        "\n",
        "In this notebook, we use the Keras_Text_Generator from the textGen module to train a GRU on text data generated from Dungeons and Dragons monster statblocks. Our goal is to create new statblocks for plausible/usable monsters using a recurrent neural network.\n",
        "\n",
        "First thing to do is import necessary modules:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlIN2NPgvCK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from textGen import Keras_Text_Generator\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dropout, Dense\n",
        "import tensorflow as tf\n",
        "#!pip install tensorflow-gpu==2.0.0-beta1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LOzzYYAtLwk",
        "colab_type": "text"
      },
      "source": [
        "Next we instantiate a text generator object that will hold our data and model and then finally output our generated text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vlZXmjmvkPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen = Keras_Text_Generator()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FXqpWQntZbR",
        "colab_type": "text"
      },
      "source": [
        "We are working with a large string that represents all of the monsters currently available in the latest edition of Dungeons and Dragons simply strung together in one continuous piece of text. Let's take a look at what we're working with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m41o-It0t7oA",
        "colab_type": "code",
        "outputId": "56385c28-9654-4c70-df6e-4b66fd072bfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with open('adjusted_full_abilities.txt', 'rb') as f:\n",
        "  print(f.read()[:100])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'<<start>>[[challenge]]1/4 {50 xp}[[strength]]10 |0|[[dexterity]]14 |+2|[[constitution]]10 |0|[[wisdo'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETtgmxy6uKl2",
        "colab_type": "text"
      },
      "source": [
        "We now call the load_and_create_dataset method which will load from the designated file and vectorize our data, as well as store it into a Tensorflow Dataset object that we will use to iterate over. We'll choose a sequence length of 500, which will create training instances that are 500 characters in length. \n",
        "\n",
        "\n",
        "I'll set the rolling sequences flag to True as we'll want to create as much data as we can. Setting this flag instructs the method to create data essentially by using a rolling window with the initial training instance consisting of the first seq_length number of characters and the second training instance starting on the second character and consisting of seq_length characters and so on. If we set this flag to false, we'd simply chop the data into seq_length size chunks.\n",
        "\n",
        "\n",
        "We'll go with a sequence length of 500 because there is a balance to be struck between creating training instances long enough for the GRU units to recognize dependencies in statblocks that can be somewhat lengthy, however we also need to think about memory cost. There are ~840,000 characters in our text string, so this will create a dataset that is a 500 x 840,000 matrix which can weigh down RAM resources. However, testing shows that a larger seq_length can be beneficial for results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCSltTlavqaR",
        "colab_type": "code",
        "outputId": "bfc19c2e-2529-49ad-e2e0-6cd79d09a2a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "gen.load_and_create_dataset('adjusted_full_abilities.txt', seq_length=500, rolling_sequences=True, \n",
        "                            rolling_sequences_step=3)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 841581 characters\n",
            "Unique characters: 67\n",
            "Dataset successfully created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR8eXLpRMCt7",
        "colab_type": "text"
      },
      "source": [
        "Next, to construct the model, we'll add each layer one at a time using the add_layer_to_model method, and by specifying the layer along with its keyword arguments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLNGjF98vw73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = gen.vocab_size\n",
        "embedding_dim = 200\n",
        "gen.add_layer_to_model(Embedding, \n",
        "                       input_dim=vocab_size, \n",
        "                       output_dim=embedding_dim)\n",
        "gen.add_layer_to_model(GRU,\n",
        "                       units=300,\n",
        "                       return_sequences=True,\n",
        "                       stateful=True,\n",
        "                       recurrent_initializer='glorot_uniform')\n",
        "gen.add_layer_to_model(Dropout,\n",
        "                      rate=0.1)\n",
        "gen.add_layer_to_model(GRU,\n",
        "                       units=300,\n",
        "                       return_sequences=True,\n",
        "                       stateful=True,\n",
        "                       recurrent_initializer='glorot_uniform')\n",
        "gen.add_layer_to_model(Dropout,\n",
        "                      rate=0.1)\n",
        "gen.add_layer_to_model(Dense,\n",
        "                      units=vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwycfaTFMdCF",
        "colab_type": "text"
      },
      "source": [
        "As you can see below, once we compile the model, there are quite a few trainable parameters. Memory units can be very compute intensive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1Row9Jexdzu",
        "colab_type": "code",
        "outputId": "358eec5a-d28a-4143-c3dd-aa63c6bbcc0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "gen.compile_model()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 200)           13400     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (64, None, 300)           451800    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (64, None, 300)           0         \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (64, None, 300)           541800    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (64, None, 300)           0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 67)            20167     \n",
            "=================================================================\n",
            "Total params: 1,027,167\n",
            "Trainable params: 1,027,167\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMCRErZ8Mryz",
        "colab_type": "text"
      },
      "source": [
        "Now we're ready to train the model. Let's call the fit_model method and see what kind of improvement in loss we can get. From experience, categorical cross entropy loss usually starts at around 4.0 before training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT73y-p3x03z",
        "colab_type": "code",
        "outputId": "f6e92925-ef2d-40b6-b8dc-14b6f8563048",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "gen.fit_model(2)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "4380/4380 [==============================] - 1596s 364ms/step - loss: 0.3503\n",
            "Epoch 2/2\n",
            "4380/4380 [==============================] - 1588s 363ms/step - loss: 0.1671\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsaqnUqHNJHz",
        "colab_type": "text"
      },
      "source": [
        "Next, we load the model, which allows us to reset the input dimensions that the model should expect. This is important because before we were using batches and the model was looking for batch_size inputs. Now however, we want to simply feed in a single text string to represent the starting sequence for our text generator. Let's take a look - the model should load with the same architecture it was trained on:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cerBQ2Bg2Lzy",
        "colab_type": "code",
        "outputId": "a26209d4-cc77-458a-fb8b-84a087c95441",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "gen.load_model_from_checkpoint()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 200)            13400     \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (1, None, 300)            451800    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (1, None, 300)            0         \n",
            "_________________________________________________________________\n",
            "gru_3 (GRU)                  (1, None, 300)            541800    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (1, None, 300)            0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 67)             20167     \n",
            "=================================================================\n",
            "Total params: 1,027,167\n",
            "Trainable params: 1,027,167\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2M09WB6No_j",
        "colab_type": "text"
      },
      "source": [
        "Looks like everything is in order. Now to get down to business. Let's generate some text using the generate_text method. We have a temperature keyword argument that we can use to control the probability distribution that we are sampling from to generate the output ASCII characters. Anything above 1.0 should generate more surprising characters, and anything less would generate less surprising characters. Let's see what we get!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VH1K-FF82XnI",
        "colab_type": "code",
        "outputId": "d6223f6e-1f31-4fe9-cfff-b498ad86896e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "gen.generate_text(temperature=0.8)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<<start>>[[challenge]]3 {700 xp}[[strength]]11 |0|[[dexterity]]18 |+4|[[constitution]]16 |+3|[[wisdom]]13 |+1|[[intelligence]]11 |0|[[charisma]]10 |0|[[armor_class]]16 (studded leather armor)[[hit_points]]75 (10d8+30)[[speed]]30 ft.[[damage_immunities]]none[[damage_resistances]]none[[condition_immunities]]none[[saving_throws]]none[[short_desc]]medium humanoid (any race), any alignment[[skills]]acrobatics +6, perception +5[[languages]]any one language (usually common)[[full_ability]]spellcasting;; the acolyte is a 1st-level spellcaster. its spellcasting ability is wisdom (spell save dc 12, +4 to hit with spell attacks). the acolyte has following cleric spells prepared:cantrips (at will): light, sacred flame, thaumaturgy+7 to hit, reach 5 ft., one target. hit: 1 (1d4 - 1) slashing damage.<<end>><<start>>[[challenge]]00[[strength]]16 |+3|[[dexterity]]16 |+3|[[constitution]]16 |+3|[[wisdom]]17 |+3|[[intelligence]]14 |+2|[[charisma]]18 |+4|[[armor_class]]29 (natural armor)[[hit_points]]333 (18d20+144)[[speed]]40 ft., burrow 20 ft., fly 60 ft.[[damage_immunities]]fire, poison[[damage_resistances]]cold; bludgeoning, piercing, and slashing from nonmagical attacks that aren't silvered[[condition_immunities]]charmed, exhaustion, frightened, paralyzed, petrified, poisoned[[saving_throws]]none[[short_desc]]large construct, unaligned[[skills]]none[[languages]]none[[full_ability]]astion and the target is parclud repains spent legendary actions at the start of its turn.bite attack;; the balhannoth makes one bite attack against one creature it has grappled.teleport;; the balhannoth magically teleports, along with any equipment it is wearing or carrying and any creatures it has grappled.teleport;; the balhannoth magically teleports, along with any equipment it is wearing or carrying and any creatures it has grappled, up to 60 feet to an unoccupied space it can see.vanish;; the balhannoth magically becomes invisible for up to 10 minutes or until immediately after it makes an attack roll.<<end\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwY_PpIgntmo",
        "colab_type": "text"
      },
      "source": [
        "Success! We have complete monsters generated by the RNN! The sequence that it generated is remarkably similar to a normal D&D monster statblock. It was able to pick up the structure of ability_name: ability_score pairs for strength, dexterity, constitution, wisdom, intelligence, charisma, etc. \n",
        "\n",
        "The curious reader will note that I set the temperature to 0.8, which means that these results should be less surprising. Without double-checking, I would be willing to guess that the monsters generated above are actually close copies of existing monsters which is less that desirable. Let's try with an increased temperature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r59n-I_5pVIc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "83982b60-bc11-432e-efa1-ffbf490447a1"
      },
      "source": [
        "gen.generate_text(temperature=1.2)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<<start>>[[challenge]]2 {450 xp}[[strength]]16 |+4|[[dexterity]]13 |+1|[[constitution]]14 |+2|[[wisdom]]11 |0|[[intelligence]]14 |+2|[[charisma]]11 |0|[[armor_class]]13[[hit_points]]20 (4d10+11)[[speed]]40 ft., burrow 5 ft.[[damage_immunities]]none[[damage_resistances]]none[[condition_immunities]]none[[saving_throws]]none[[short_desc]]large beast, unaligned[[skills]]none[[languages]]none[[full_ability]]tail;; melee weapon attack: +7 to hit, reach 10 ft., one target. hit: 18 (4d6 + 4) bludgeoning damage. if the target is a creature, it must succeed on a dc 15 strength saving throw or be knocked prone.<<end>><<start>>[[challenge]]2 {450 xp}[[strength]]17 |+3|[[dexterity]]11 |0|[[constitution]]13 |+1|[[wisdom]]13 |+1|[[intelligence]]1 |-5|[[charisma]]6 |-2|[[armor_class]]14 (natural armor, 11 while prone)[[hit_points]]39 (6d10+6)[[speed]]30 ft., burrow 15 ft., swim 40 ft.[[damage_immunities]]none[[damage_resistances]]none[[condition_immunities]]none[[saving_throws]]none[[short_desc]]medium monstros]]none[[saving_throws]]none[[short_desc]]large one target. hit: 7 (2d6 + 3) slashing damage plus 33 (6d10) force damage.disruptive touch;; melee spell attack: +11 to hit, reach 5 ft., one target. hit: 44 (8d10) nenation +4, persuasion)[[short_desc]]medium humanoid (human), neutral evil[[skills]]intimided by its turn insige on ally hit ting that is 5 feet wide. each creature in that line must make a dc 13 dexterity saving throw, taking 10 (3d6) acid damage on a failed save, or half as much damage on a successful one. if th s must succeed on a dc 14 wisdom saving throw, or it hears a faint buzzing in its hears. each creature in that languages it knew in life[[full_asting throw. on a failed save, a thrieldrumannity]]fire fire (recharge 6);; the ankheg spits acid in that is 30 ft.[[damage_immunities]]cold[[damage_resistarces]]none[[condition_immunities]]none[[saving_throws]]dex +4, wis +3[[short_desc]]medium humanoid (any race), any alignment[[skills]]arcana +6, history +6[[languages]]any'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMokzPmhpwR3",
        "colab_type": "text"
      },
      "source": [
        "The higher temperature is still surprisingly true to the structure of a statblock. However, upon further examination, the string has some skips where the generator samples a \"more surprising\" character which feel more \"out of sequence\" than surprising and the GRU snaps into a new sequence seemingly all of a sudden. I would describe this as a mistake. When this happens, the statblock is not more surprising, but rather unusable. This marks an area for improvement in the future."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0A47uZBqdSMk",
        "colab_type": "text"
      },
      "source": [
        "## Final Thoughts\n",
        "\n",
        "We got some truly fantastic results. The RNNs were able to very closely mimic the strange syntax of a D&D statblock. It was even able to accomplish dice math with a relatively high degree of reliability when determining statistics like hit points. And did so without any mathematical formulas!\n",
        "\n",
        "We did discover that we have a little bit of a catch-22 when generating text: when we set the temperature lower, we find near copies of existing statblocks, but when we set it higher, we risk the generator breaking syntax. We could continue to try and solve this problem with hyperparameter tuning as well as different variations on the neural network's architecture. Another solution might be to try using word level generators. This would allow for transfer learning, but would potentially limit the vocabulary of the generator. Regardless, with recent advances in transfer learning in NLP, it seems to demand a test. \n",
        "\n",
        "More to come! Stay tuned!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9cj5ISSZ1Vz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}